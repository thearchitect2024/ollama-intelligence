{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded GPU Qwen2.5-7B Intelligence Extraction - SageMaker Studio\n",
    "\n",
    "This notebook runs the Streamlit app with **embedded GPU inference** (no Ollama needed).\n",
    "\n",
    "**Branch:** `embedded-microbatch`\n",
    "\n",
    "---\n",
    "\n",
    "## Features:\n",
    "- ‚ö° Direct GPU inference (no REST API overhead)\n",
    "- üß± 4-bit quantization (bitsandbytes)\n",
    "- üöÄ Micro-batching for higher throughput  \n",
    "- üí® FlashAttention2 / SDPA for optimized attention\n",
    "- üåê Public URL via ngrok\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements:\n",
    "- GPU-enabled SageMaker instance (ml.g4dn.xlarge or better)\n",
    "- ~8GB GPU memory minimum\n",
    "- ~5GB disk space for model cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install -q streamlit pyngrok pandas pydantic pydantic-settings python-dotenv py-spy\n",
    "!pip install -q torch transformers accelerate bitsandbytes einops\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(\"   Streamlit, torch, transformers, accelerate, bitsandbytes, einops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Ngrok Token\n",
    "\n",
    "**Get your free token:** https://dashboard.ngrok.com/get-started/your-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE THIS WITH YOUR ACTUAL NGROK TOKEN\n",
    "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
    "\n",
    "if NGROK_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
    "    print(\"‚ùå ERROR: Set your ngrok token above!\")\n",
    "    print(\"   Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "else:\n",
    "    print(f\"‚úÖ Ngrok token configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Download Qwen2.5-7B Model (First Run Only)\n",
    "\n",
    "This downloads and caches the model (~4.3 GB). Subsequent runs will be instant.\n",
    "\n",
    "**Time:** 5-10 minutes on first run, instant after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üì• PRE-DOWNLOADING QWEN2.5-7B-INSTRUCT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "USE_4BIT = True\n",
    "\n",
    "print(f\"\\nüì¶ Model: {MODEL_NAME}\")\n",
    "print(f\"üîß 4-bit quantization: {USE_4BIT}\")\n",
    "print(f\"üíæ Cache: ~/.cache/huggingface/hub/\")\n",
    "print(f\"üñ•Ô∏è  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - model will run on CPU (slower)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Downloading Tokenizer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(\"‚úÖ Tokenizer downloaded (~1 MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Downloading Model Weights (~4.3 GB)\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚è≥ This will take 5-10 minutes on first run...\")\n",
    "print(\"   Subsequent runs will be instant (cached)\\n\")\n",
    "\n",
    "if USE_4BIT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    print(\"üß± Loading with 4-bit quantization (bitsandbytes)\")\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"‚úÖ Model downloaded and quantized to 4-bit\")\n",
    "    print(f\"   Memory usage: ~2.7 GB\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"‚úÖ Model downloaded (FP16)\")\n",
    "    print(f\"   Memory usage: ~14 GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Testing Generation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "inputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, temperature=0.7)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ Test generation successful!\")\n",
    "print(f\"   Input: 'Hello, I am'\")\n",
    "print(f\"   Output: '{result}'\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del model\n",
    "del tokenizer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ MODEL READY FOR EMBEDDED GPU INFERENCE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Model: {MODEL_NAME}\")\n",
    "print(f\"‚úÖ Quantization: {'4-bit (NF4)' if USE_4BIT else 'FP16'}\")\n",
    "print(f\"‚úÖ Cache: ~/.cache/huggingface/hub/\")\n",
    "print(f\"‚úÖ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"\\nStreamlit app will use this cached model for instant startup! üöÄ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create .env file\n",
    "with open('.env', 'w') as f:\n",
    "    f.write('''DATABASE_URL=sqlite:////tmp/contributor_intelligence.db\n",
    "\n",
    "# Embedded GPU config\n",
    "EMBED_MODEL=Qwen/Qwen2.5-7B-Instruct\n",
    "EMBED_4BIT=1\n",
    "INFER_CONCURRENCY=8\n",
    "MICRO_BATCH_SIZE=32\n",
    "BATCH_LATENCY_MS=120\n",
    "''')\n",
    "\n",
    "print(\"‚úÖ Environment configured (SQLite + Embedded GPU)\")\n",
    "\n",
    "# Create SQLite database\n",
    "db_path = '/tmp/contributor_intelligence.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS contributors (\n",
    "        email TEXT PRIMARY KEY,\n",
    "        contributor_id TEXT UNIQUE NOT NULL,\n",
    "        processed_data TEXT NOT NULL,\n",
    "        intelligence_summary TEXT,\n",
    "        processing_status TEXT DEFAULT 'pending',\n",
    "        error_message TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        intelligence_extracted_at TIMESTAMP\n",
    "    )\n",
    "''')\n",
    "conn.execute('CREATE INDEX IF NOT EXISTS idx_status ON contributors(processing_status)')\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"‚úÖ SQLite database: {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok, conf\n",
    "\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "print(\"‚úÖ Ngrok configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Stop any existing Streamlit\n",
    "subprocess.run(['pkill', '-f', 'streamlit'], capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "# Start Streamlit\n",
    "print(\"üé® Starting Streamlit app with embedded GPU...\")\n",
    "with open('/tmp/streamlit.log', 'w') as log:\n",
    "    streamlit_process = subprocess.Popen(\n",
    "        ['streamlit', 'run', 'app.py', \n",
    "         '--server.port', '8501',\n",
    "         '--server.headless', 'true'],\n",
    "        stdout=log,\n",
    "        stderr=log\n",
    "    )\n",
    "\n",
    "time.sleep(8)\n",
    "print(f\"‚úÖ Streamlit started (PID: {streamlit_process.pid})\")\n",
    "print(\"‚è≥ Model loading in background (first request will take ~30s)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Public URL üåê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ngrok.kill()\n",
    "    time.sleep(2)\n",
    "    public_url = ngrok.connect(8501, bind_tls=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üåê PUBLIC URL READY!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüîó {public_url}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EMBEDDED GPU FEATURES:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úÖ 4-bit quantization (bitsandbytes)\")\n",
    "    print(\"‚úÖ Micro-batching (32 batch, 120ms latency)\")\n",
    "    print(\"‚úÖ Semaphore concurrency (8 slots)\")\n",
    "    print(\"‚úÖ FlashAttention2 / SDPA\")\n",
    "    print(\"‚úÖ NO Ollama server needed!\")\n",
    "    print(\"\\n‚ö†Ô∏è  Keep this cell running!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stop Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ngrok.kill()\n",
    "subprocess.run(['pkill', '-f', 'streamlit'], capture_output=True)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ All services stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
