"""
Optimized embedded GPU LLM engine for Qwen 2.5 7B Instruct (Q4_0).

HYBRID ARCHITECTURE (Best of Both Worlds):
- Queue + Dispatcher thread with latency window (streaming model)
- Length-aware bucketing (efficiency optimization)
- Per-request Futures (async blocking)
- Semaphore-controlled GPU concurrency (prevents OOM)

Features:
- FlashAttention-2 with SDPA fallback
- Micro-batching with 50ms latency window
- Length-aware bucketing for padding reduction
- Pinned memory + dual CUDA streams
- torch.compile for graph optimization
- 10 concurrent GPU batches (tested safe on L4)

FLOW:
1. App threads submit requests to queue â†’ get Future
2. Dispatcher collects requests over 50ms window
3. Groups into length-aware buckets (31 items each)
4. Semaphore controls GPU concurrency (10 max)
5. GPU processes batch â†’ results set on Futures
6. App threads unblock and continue
"""
import os
import time
import queue
import logging
import threading
from typing import List, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict
from concurrent.futures import Future

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

logger = logging.getLogger(__name__)

# =============================================================================
# Configuration (from environment or defaults)
# =============================================================================
EMBED_MODEL = os.getenv("EMBED_MODEL", "Qwen/Qwen2.5-7B-Instruct")
EMBED_4BIT = os.getenv("EMBED_4BIT", "1") == "1"
INFER_CONCURRENCY = int(os.getenv("INFER_CONCURRENCY", "3"))
MICRO_BATCH_SIZE = int(os.getenv("MICRO_BATCH_SIZE", "6"))
BATCH_LATENCY_MS = int(os.getenv("BATCH_LATENCY_MS", "100"))
PREFILL_BATCH_TOKENS = int(os.getenv("PREFILL_BATCH_TOKENS", "4096"))
DECODE_CONCURRENCY = int(os.getenv("DECODE_CONCURRENCY", "8"))
USE_FLASH_ATTENTION = os.getenv("USE_FLASH_ATTENTION", "True").lower() == "true"
ENABLE_COMPILE = os.getenv("ENABLE_COMPILE", "True").lower() == "true"
MAX_INPUT_TOK = int(os.getenv("MAX_INPUT_TOK", "4096"))


@dataclass
class BatchMetrics:
    """Metrics for a single batch inference."""
    batch_size: int
    total_input_tokens: int
    avg_input_len: float
    prefill_time_ms: float
    decode_time_ms: float
    tokens_per_sec: float
    vram_current_gb: float
    vram_peak_gb: float


# =============================================================================
# Model Initialization
# =============================================================================
_model = None
_tokenizer = None
_copy_stream = None
_compute_stream = None
_lock = threading.Lock()
_semaphore = threading.BoundedSemaphore(INFER_CONCURRENCY)


def _enable_gpu_optimizations():
    """Enable GPU performance optimizations."""
    try:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.set_float32_matmul_precision("high")
        logger.info("âœ… GPU optimizations enabled (TF32, cuDNN benchmark)")
    except Exception as e:
        logger.warning(f"Failed to enable GPU optimizations: {e}")


def _check_flash_attention():
    """Check FlashAttention-2 availability."""
    if not USE_FLASH_ATTENTION:
        logger.info("â„¹ï¸  FlashAttention disabled via config")
        return "sdpa"
    
    try:
        import flash_attn
        logger.info("âœ… FlashAttention-2 available")
        return "flash_attention_2"
    except ImportError:
        logger.info("â„¹ï¸  FlashAttention-2 not installed, using SDPA")
        return "sdpa"


def _init_model(temperature: float, top_p: float, max_tokens: int):
    """Initialize the model singleton with optimizations."""
    global _model, _tokenizer, _copy_stream, _compute_stream
    
    if _model is not None:
        return
    
    with _lock:
        if _model is not None:
            return
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        if device == "cpu":
            logger.warning("âš ï¸  No CUDA available, running on CPU (slow!)")
        
        # GPU optimizations
        _enable_gpu_optimizations()
        
        # Determine attention implementation
        attn_impl = _check_flash_attention()
        
        # Model loading kwargs
        model_kwargs = {
            "device_map": "auto",
            "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            "low_cpu_mem_usage": True,
            "trust_remote_code": True,
            "use_cache": True,
        }
        
        # Add attention implementation
        try:
            model_kwargs["attn_implementation"] = attn_impl
        except Exception:
            logger.warning("attn_implementation not supported, skipping")
        
        # 4-bit quantization (if enabled)
        if EMBED_4BIT and device == "cuda":
            try:
                from transformers import BitsAndBytesConfig
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True
                )
                logger.info("ðŸ§± Loading with 4-bit quantization")
            except Exception as e:
                logger.warning(f"4-bit quantization failed: {e}, using FP16")
        
        logger.info(f"ðŸš€ Loading {EMBED_MODEL} ({attn_impl})...")
        
        # Load model
        _model = AutoModelForCausalLM.from_pretrained(EMBED_MODEL, **model_kwargs)
        _model.eval()
        
        # Try torch.compile (PyTorch 2.0+)
        if ENABLE_COMPILE and device == "cuda":
            try:
                logger.info("âš™ï¸  Compiling model with torch.compile...")
                _model = torch.compile(
                    _model,
                    mode="reduce-overhead",
                    fullgraph=False  # More robust
                )
                logger.info("âœ… Model compiled")
            except Exception as e:
                logger.info(f"â„¹ï¸  torch.compile not available: {e}")
        
        # Load tokenizer
        _tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL, trust_remote_code=True)
        if _tokenizer.pad_token is None:
            _tokenizer.pad_token = _tokenizer.eos_token
        
        # Initialize CUDA streams for overlap
        if device == "cuda":
            _copy_stream = torch.cuda.Stream()
            _compute_stream = torch.cuda.Stream()
            logger.info("âœ… Dual CUDA streams initialized")
        
        # Warmup
        try:
            with torch.inference_mode():
                dummy = _tokenizer("test", return_tensors="pt")
                if device == "cuda":
                    dummy = {k: v.to(device) for k, v in dummy.items()}
                _ = _model.generate(**dummy, max_new_tokens=1)
            logger.info("âœ… Model warmed up")
        except Exception as e:
            logger.warning(f"Warmup failed: {e}")
        
        logger.info(f"âœ… Embedded LLM ready | VRAM: {_get_vram_usage():.1f} GB")


# =============================================================================
# Memory & Performance Utilities
# =============================================================================
def _get_vram_usage() -> float:
    """Get current VRAM usage in GB."""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024**3
    return 0.0


def _get_vram_peak() -> float:
    """Get peak VRAM usage in GB."""
    if torch.cuda.is_available():
        return torch.cuda.max_memory_allocated() / 1024**3
    return 0.0


def _reset_vram_peak():
    """Reset peak VRAM counter."""
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()


# =============================================================================
# Micro-Batch Dispatcher (Queue + Latency Window)
# =============================================================================
class MicroBatchDispatcher:
    """
    Continuous dispatcher that collects requests over a latency window.
    
    Similar to the reference architecture but adds length-aware bucketing.
    """
    
    def __init__(self, max_batch_size: int, latency_ms: int, semaphore: threading.BoundedSemaphore):
        self.max_batch_size = max_batch_size
        self.latency_ms = latency_ms
        self.semaphore = semaphore
        self.q = queue.Queue()
        self._stopping = False
        self._thread = threading.Thread(target=self._loop, name="mb-dispatcher", daemon=True)
        self._thread.start()
        logger.info(f"ðŸš€ Dispatcher started: batch_size={max_batch_size}, latency={latency_ms}ms")
    
    def submit(self, prompt: str) -> str:
        """
        Submit a prompt for generation. Blocks until result ready.
        
        Args:
            prompt: Input prompt string
            
        Returns:
            Generated text
        """
        fut = Future()
        self.q.put((prompt, fut))
        return fut.result()  # Block until dispatcher processes and sets result
    
    def _loop(self):
        """Dispatcher loop: collect requests over latency window, then process."""
        while not self._stopping:
            try:
                # Wait for first item (blocking)
                first_item = self.q.get(timeout=0.1)
                batch = [first_item]
                
                # Set deadline (current_time + latency_ms)
                deadline = time.perf_counter() + (self.latency_ms / 1000.0)
                
                # Collect more items until deadline OR batch full
                while len(batch) < self.max_batch_size:
                    remaining = deadline - time.perf_counter()
                    if remaining <= 0:
                        break  # Time's up!
                    
                    try:
                        item = self.q.get(timeout=remaining)
                        batch.append(item)
                    except queue.Empty:
                        break  # No more items
                
                # Process batch
                if batch:
                    self._process_batch(batch)
                    
            except queue.Empty:
                continue  # No items, wait for next
            except Exception as e:
                logger.error(f"Dispatcher error: {e}")
    
    def _process_batch(self, items: List[Tuple[str, Future]]):
        """
        Process a batch with length-aware bucketing and GPU execution.
        
        Args:
            items: List of (prompt, future) tuples
        """
        prompts = [item[0] for item in items]
        futures = [item[1] for item in items]
        
        # Length-aware bucketing for efficiency
        # Group prompts by similar length to reduce padding
        indexed = [(i, p, len(_tokenizer.encode(p, add_special_tokens=False))) 
                   for i, p in enumerate(prompts)]
        indexed.sort(key=lambda x: x[2])  # Sort by length
        
        # Create length buckets
        buckets = []
        for i in range(0, len(indexed), self.max_batch_size):
            bucket_items = indexed[i:i + self.max_batch_size]
            bucket_indices = [item[0] for item in bucket_items]
            bucket_prompts = [item[1] for item in bucket_items]
            buckets.append((bucket_indices, bucket_prompts))
        
        # Process each bucket with semaphore control
        for bucket_indices, bucket_prompts in buckets:
            with self.semaphore:  # GPU concurrency control
                try:
                    # Generate for this bucket
                    results, _ = _generate_batch_optimized(
                        bucket_prompts,
                        max_new_tokens=320,
                        temperature=0.05,
                        top_p=0.9
                    )
                    
                    # Set results on corresponding futures
                    for idx, result in zip(bucket_indices, results):
                        futures[idx].set_result(result)
                        
                except Exception as e:
                    logger.error(f"Batch processing error: {e}")
                    # Set error on all futures in this bucket
                    for idx in bucket_indices:
                        futures[idx].set_exception(e)
    
    def shutdown(self):
        """Gracefully shutdown the dispatcher."""
        self._stopping = True
        self._thread.join(timeout=2.0)


# =============================================================================
# Length-Aware Bucketing
# =============================================================================
def _bucket_by_length(prompts: List[str], bucket_size: int = 8) -> List[List[Tuple[int, str]]]:
    """
    Sort prompts by length and group into buckets.
    Returns list of buckets, each bucket is [(index, prompt), ...]
    """
    # Get lengths
    indexed = [(i, p, len(_tokenizer.encode(p, add_special_tokens=False))) 
               for i, p in enumerate(prompts)]
    
    # Sort by length
    indexed.sort(key=lambda x: x[2])
    
    # Group into buckets
    buckets = []
    for i in range(0, len(indexed), bucket_size):
        bucket = [(idx, prompt) for idx, prompt, _ in indexed[i:i + bucket_size]]
        buckets.append(bucket)
    
    return buckets


# =============================================================================
# Optimized Batch Generation
# =============================================================================
def _generate_batch_optimized(
    texts: List[str],
    max_new_tokens: int,
    temperature: float,
    top_p: float
) -> Tuple[List[str], BatchMetrics]:
    """
    Generate with GPU optimizations and metrics.
    """
    batch_size = len(texts)
    _reset_vram_peak()
    
    # Tokenize
    start_time = time.time()
    enc = _tokenizer(
        texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_INPUT_TOK
    )
    
    total_input_tokens = enc['input_ids'].numel()
    avg_input_len = total_input_tokens / batch_size
    
    # Transfer with pinned memory + non-blocking
    if torch.cuda.is_available():
        # Pin memory
        for k in enc:
            if not enc[k].is_pinned():
                enc[k] = enc[k].pin_memory()
        
        # Transfer with copy stream
        with torch.cuda.stream(_copy_stream):
            enc = {k: v.to(_model.device, non_blocking=True) for k, v in enc.items()}
        
        # Wait for copy
        torch.cuda.current_stream().wait_stream(_copy_stream)
    else:
        enc = {k: v.to(_model.device) for k, v in enc.items()}
    
    prefill_time = (time.time() - start_time) * 1000
    
    # Generate with compute stream
    decode_start = time.time()
    
    with _semaphore:
        if torch.cuda.is_available():
            with torch.cuda.stream(_compute_stream):
                with torch.inference_mode():
                    out = _model.generate(
                        **enc,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=True,
                        pad_token_id=_tokenizer.eos_token_id,
                        eos_token_id=_tokenizer.eos_token_id,
                        use_cache=True,
                    )
            torch.cuda.current_stream().wait_stream(_compute_stream)
            torch.cuda.synchronize()
        else:
            with torch.inference_mode():
                out = _model.generate(
                    **enc,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=_tokenizer.eos_token_id,
                    eos_token_id=_tokenizer.eos_token_id,
                    use_cache=True,
                )
    
    decode_time = (time.time() - decode_start) * 1000
    total_time = prefill_time + decode_time
    
    # Decode
    decoded = _tokenizer.batch_decode(out, skip_special_tokens=True)
    
    # Clean up echo
    cleaned = []
    for text, prompt in zip(decoded, texts):
        text = text.strip()
        if text.startswith(prompt[:64]):
            text = text[len(prompt):].strip()
        cleaned.append(text)
    
    # Metrics
    total_output_tokens = out.numel() - total_input_tokens
    tokens_per_sec = total_output_tokens / (total_time / 1000) if total_time > 0 else 0
    
    metrics = BatchMetrics(
        batch_size=batch_size,
        total_input_tokens=total_input_tokens,
        avg_input_len=avg_input_len,
        prefill_time_ms=prefill_time,
        decode_time_ms=decode_time,
        tokens_per_sec=tokens_per_sec,
        vram_current_gb=_get_vram_usage(),
        vram_peak_gb=_get_vram_peak()
    )
    
    # Log metrics
    logger.info(
        f"ðŸ“Š Batch: size={batch_size}, input_tokens={total_input_tokens}, "
        f"avg_len={avg_input_len:.0f}, prefill={prefill_time:.1f}ms, "
        f"decode={decode_time:.1f}ms, tok/s={tokens_per_sec:.1f}, "
        f"vram={metrics.vram_current_gb:.1f}GB (peak={metrics.vram_peak_gb:.1f}GB)"
    )
    
    return cleaned, metrics


# =============================================================================
# Public API
# =============================================================================
class EmbeddedLLMEngine:
    """
    Optimized embedded LLM engine with streaming dispatcher.
    
    Uses Queue + Dispatcher thread for continuous request collection.
    """
    
    def __init__(self, temperature: float, top_p: float, max_tokens: int):
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        self._semaphore = _semaphore  # Use global semaphore
        
        # Initialize model
        _init_model(temperature, top_p, max_tokens)
        
        # Initialize dispatcher
        self._dispatcher = MicroBatchDispatcher(
            max_batch_size=MICRO_BATCH_SIZE,
            latency_ms=BATCH_LATENCY_MS,
            semaphore=self._semaphore
        )
        
        logger.info(
            f"ðŸš€ EmbeddedLLMEngine initialized | "
            f"concurrency={INFER_CONCURRENCY}, batch_size={MICRO_BATCH_SIZE}, "
            f"latency={BATCH_LATENCY_MS}ms, flash_attn={USE_FLASH_ATTENTION}"
        )
    
    def generate(self, prompt: str) -> str:
        """
        Generate for a single prompt using streaming dispatcher.
        
        This is the main method for per-request processing.
        Submits to queue and blocks until result ready.
        
        Args:
            prompt: Input prompt
            
        Returns:
            Generated text
        """
        return self._dispatcher.submit(prompt)
    
    def generate_single(self, prompt: str) -> str:
        """Alias for generate() for backward compatibility."""
        return self.generate(prompt)
    
    def generate_batch(self, prompts: List[str], progress_callback=None) -> Tuple[List[str], List[BatchMetrics]]:
        """
        Generate for multiple prompts with length-aware bucketing and concurrent processing.
        
        Args:
            prompts: List of prompts to process
            progress_callback: Optional callback(completed, total) for progress updates
            
        Returns:
            (results, metrics_per_bucket)
        """
        if not prompts:
            return [], []
        
        if len(prompts) == 1:
            results, metrics = _generate_batch_optimized(
                prompts, self.max_tokens, self.temperature, self.top_p
            )
            if progress_callback:
                progress_callback(1, 1)
            return results, [metrics]
        
        # Bucket by length
        buckets = _bucket_by_length(prompts, bucket_size=MICRO_BATCH_SIZE)
        logger.info(f"ðŸ“¦ Split {len(prompts)} prompts into {len(buckets)} buckets")
        
        # Process buckets concurrently
        all_results = [None] * len(prompts)
        all_metrics = []
        completed_buckets = [0]  # Mutable for closure
        
        def process_bucket(bucket):
            """Process a single bucket with semaphore control."""
            indices, bucket_prompts = zip(*bucket)
            
            # Semaphore controls GPU concurrency
            with self._semaphore:
                bucket_results, metrics = _generate_batch_optimized(
                    list(bucket_prompts),
                    self.max_tokens,
                    self.temperature,
                    self.top_p
                )
            
            return indices, bucket_results, metrics
        
        # Use ThreadPoolExecutor for concurrent bucket processing
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        with ThreadPoolExecutor(max_workers=self.infer_concurrency) as executor:
            # Submit all buckets
            futures = {executor.submit(process_bucket, bucket): i for i, bucket in enumerate(buckets)}
            
            # Collect results as they complete
            for future in as_completed(futures):
                bucket_idx = futures[future]
                try:
                    indices, bucket_results, metrics = future.result()
                    
                    # Map back to original indices
                    for idx, result in zip(indices, bucket_results):
                        all_results[idx] = result
                    
                    all_metrics.append(metrics)
                    
                    # Update progress
                    completed_buckets[0] += 1
                    if progress_callback:
                        progress_callback(completed_buckets[0], len(buckets))
                        
                except Exception as e:
                    logger.error(f"Bucket {bucket_idx} failed: {e}")
                    # Fill with error messages for this bucket
                    bucket = buckets[bucket_idx]
                    for idx, _ in bucket:
                        all_results[idx] = f"Error: {e}"
        
        return all_results, all_metrics
    
    def is_available(self) -> bool:
        """Check if engine is ready."""
        return _model is not None and _tokenizer is not None

