{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Streamlit App in SageMaker Studio with Public URL\n",
        "\n",
        "This notebook starts your Streamlit app and creates a public URL using ngrok.\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 Required Files\n",
        "\n",
        "Make sure you have uploaded:\n",
        "- `app.py` (your Streamlit app)\n",
        "- `src/` directory (all modules)\n",
        "- `requirements.txt`\n",
        "- This notebook\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Before You Start\n",
        "\n",
        "Get your free ngrok token:\n",
        "1. Visit: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "2. Sign up (free)\n",
        "3. Copy your token\n",
        "4. Paste it in **Cell 2** below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies (including py-spy for profiling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q streamlit pyngrok psycopg2-binary pandas pydantic pydantic-settings langchain-ollama python-dotenv py-spy\n",
        "\n",
        "print(\"✅ Dependencies installed (including py-spy for flamegraph profiling)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Ngrok Token\n",
        "\n",
        "**⚠️ REPLACE `YOUR_NGROK_TOKEN_HERE` with your token from https://dashboard.ngrok.com**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# REPLACE THIS WITH YOUR ACTUAL NGROK TOKEN\n",
        "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
        "\n",
        "if NGROK_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"❌ ERROR: Set your ngrok token above!\")\n",
        "    print(\"   Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "else:\n",
        "    print(f\"✅ Ngrok token configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install and Start Ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Install Ollama if needed\n",
        "try:\n",
        "    result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"✅ Ollama already installed\")\n",
        "    else:\n",
        "        raise FileNotFoundError\n",
        "except:\n",
        "    print(\"📦 Installing Ollama...\")\n",
        "    subprocess.run('curl -fsSL https://ollama.com/install.sh | sh', shell=True)\n",
        "    print(\"✅ Ollama installed\")\n",
        "\n",
        "# Stop existing processes\n",
        "subprocess.run(['pkill', 'ollama'], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Ollama with optimized settings\n",
        "print(\"🚀 Starting Ollama server...\")\n",
        "ollama_env = os.environ.copy()\n",
        "ollama_env['OLLAMA_NUM_PARALLEL'] = '10'\n",
        "ollama_env['OLLAMA_MAX_LOADED_MODELS'] = '1'\n",
        "\n",
        "with open('/tmp/ollama.log', 'w') as log:\n",
        "    ollama_process = subprocess.Popen(['ollama', 'serve'], \n",
        "                                      env=ollama_env,\n",
        "                                      stdout=log, \n",
        "                                      stderr=log)\n",
        "\n",
        "time.sleep(5)\n",
        "print(f\"✅ Ollama started (PID: {ollama_process.pid})\")\n",
        "\n",
        "# Download model if needed\n",
        "print(\"📥 Checking for model...\")\n",
        "result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
        "if 'qwen2.5:7b-instruct-q4_0' not in result.stdout:\n",
        "    print(\"📥 Downloading Qwen 2.5 7B (5-10 min)...\")\n",
        "    subprocess.run(['ollama', 'pull', 'qwen2.5:7b-instruct-q4_0'])\n",
        "    print(\"✅ Model downloaded\")\n",
        "else:\n",
        "    print(\"✅ Model ready\")\n",
        "\n",
        "print(\"\\\\n✅ Ollama ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup SQLite Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "# Create .env file for app with SQLite configuration\n",
        "with open('.env', 'w') as f:\n",
        "    f.write('''DATABASE_URL=sqlite:////tmp/contributor_intelligence.db\n",
        "\n",
        "OLLAMA_MODEL=qwen2.5:7b-instruct-q4_0\n",
        "OLLAMA_BASE_URL=http://localhost:11434\n",
        "''')\n",
        "\n",
        "print(\"✅ Environment configured (SQLite mode)\")\n",
        "\n",
        "# Create SQLite database\n",
        "db_path = '/tmp/contributor_intelligence.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "conn.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS contributors (\n",
        "        email TEXT PRIMARY KEY,\n",
        "        contributor_id TEXT UNIQUE NOT NULL,\n",
        "        processed_data TEXT NOT NULL,\n",
        "        intelligence_summary TEXT,\n",
        "        processing_status TEXT DEFAULT 'pending',\n",
        "        error_message TEXT,\n",
        "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "        intelligence_extracted_at TIMESTAMP\n",
        "    )\n",
        "''')\n",
        "conn.execute('CREATE INDEX IF NOT EXISTS idx_status ON contributors(processing_status)')\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(f\"✅ SQLite database: {db_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure Ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().auth_token = NGROK_TOKEN\n",
        "print(\"✅ Ngrok configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Start Streamlit App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop existing Streamlit\n",
        "subprocess.run(['pkill', '-f', 'streamlit'], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit\n",
        "print(\"🎨 Starting Streamlit app...\")\n",
        "with open('/tmp/streamlit.log', 'w') as log:\n",
        "    streamlit_process = subprocess.Popen(\n",
        "        ['streamlit', 'run', 'app.py', \n",
        "         '--server.port', '8501',\n",
        "         '--server.headless', 'true'],\n",
        "        stdout=log,\n",
        "        stderr=log\n",
        "    )\n",
        "\n",
        "time.sleep(8)\n",
        "print(f\"✅ Streamlit started (PID: {streamlit_process.pid})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Public URL 🌐\n",
        "\n",
        "**🎉 This creates your public URL! Keep this cell running!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Monitor Streamlit Logs (Optional)\n",
        "\n",
        "Run this cell to see live logs from Streamlit app.\n",
        "Keep it running while you use the app to see what's happening.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor Streamlit logs in real-time\n",
        "# Press STOP button to exit\n",
        "\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(\"📊 Monitoring Streamlit logs...\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Press the STOP button (■) to exit\\n\")\n",
        "\n",
        "try:\n",
        "    # Clear the log file first to start fresh\n",
        "    with open('/tmp/streamlit.log', 'w') as f:\n",
        "        f.write('')\n",
        "    \n",
        "    line_count = 0\n",
        "    while True:\n",
        "        with open('/tmp/streamlit.log', 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        # Only show new lines\n",
        "        if len(lines) > line_count:\n",
        "            for line in lines[line_count:]:\n",
        "                print(line.rstrip())\n",
        "            line_count = len(lines)\n",
        "        \n",
        "        time.sleep(0.5)  # Update every 0.5 seconds\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n✅ Stopped monitoring logs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"✅ YOUR STREAMLIT APP IS NOW PUBLIC!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n🌐 Public URL: {public_url}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"\\n📊 Open this URL in your browser!\")\n",
        "print(\"\\n⚠️  Keep this notebook running!\")\n",
        "print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "PUBLIC_URL = str(public_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 🔥 PERFORMANCE PROFILING\n",
        "\n",
        "## 8. Profile Intelligence Extraction with py-spy\n",
        "\n",
        "Creates a flamegraph in speedscope format!\n",
        "\n",
        "**Run this AFTER uploading CSV data in the Streamlit app**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create profiling script\n",
        "profiling_script = '''\n",
        "import sys, os\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "import asyncio\n",
        "from src.config import get_settings\n",
        "from src.database.connection import DatabaseManager\n",
        "from src.database.repositories import ContributorRepository\n",
        "from src.intelligence.llm_client import OllamaClient\n",
        "from src.intelligence.skill_extractor import (\n",
        "    has_project_descriptions,\n",
        "    generate_summary_no_projects,\n",
        "    generate_summary_no_descriptions,\n",
        "    parse_llm_response\n",
        ")\n",
        "from src.models import ContributorProfile\n",
        "\n",
        "async def profile_extraction():\n",
        "    settings = get_settings()\n",
        "    db_manager = DatabaseManager(settings)\n",
        "    repo = ContributorRepository(db_manager)\n",
        "    llm_client = OllamaClient(settings)\n",
        "    \n",
        "    pending = repo.get_contributors_without_intelligence()\n",
        "    if not pending:\n",
        "        print(\"No pending contributors\")\n",
        "        return\n",
        "    \n",
        "    pending = pending[:100]  # Profile first 100\n",
        "    print(f\"Profiling {len(pending)} contributors...\")\n",
        "    \n",
        "    processed = 0\n",
        "    failed = 0\n",
        "    batch_size = settings.max_concurrent_llm\n",
        "    \n",
        "    # Process in batches\n",
        "    for batch_start in range(0, len(pending), batch_size):\n",
        "        batch_end = min(batch_start + batch_size, len(pending))\n",
        "        batch = pending[batch_start:batch_end]\n",
        "        \n",
        "        # Convert to profiles\n",
        "        profiles = []\n",
        "        for contrib in batch:\n",
        "            try:\n",
        "                profile = ContributorProfile(**contrib[\"processed_data\"])\n",
        "                profiles.append(profile)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to parse profile: {e}\")\n",
        "                failed += 1\n",
        "        \n",
        "        # Separate by description availability\n",
        "        profiles_with_descriptions = []\n",
        "        profiles_without_descriptions = []\n",
        "        \n",
        "        for profile in profiles:\n",
        "            if not profile.production_projects:\n",
        "                summary = generate_summary_no_projects(profile)\n",
        "                profile.extracted_skills = []\n",
        "                profiles_without_descriptions.append((profile, summary))\n",
        "            elif not has_project_descriptions(profile):\n",
        "                summary = generate_summary_no_descriptions(profile)\n",
        "                profile.extracted_skills = []\n",
        "                profiles_without_descriptions.append((profile, summary))\n",
        "            else:\n",
        "                profiles_with_descriptions.append(profile)\n",
        "        \n",
        "        # Process profiles WITH descriptions using LLM\n",
        "        if profiles_with_descriptions:\n",
        "            prompt_texts = []\n",
        "            for profile in profiles_with_descriptions:\n",
        "                # Build prompt (simplified version)\n",
        "                prompt = f\"\"\"You MUST output in this EXACT format:\n",
        "\n",
        "SUMMARY:\n",
        "[Your 90-120 word paragraph here]\n",
        "\n",
        "SKILLS:\n",
        "- Skill 1\n",
        "- Skill 2\n",
        "\n",
        "===== CONTRIBUTOR DATA =====\n",
        "Location: {str(profile.location)}\n",
        "Languages: {\", \".join([lang.language for lang in profile.languages[:3]]) if profile.languages else \"Not specified\"}\n",
        "Education: {profile.education_level if profile.education_level else \"Not specified\"}\n",
        "Production Projects: {profile.activity_summary.total_production_projects}\n",
        "\n",
        "Project Descriptions:\n",
        "{\"\\\\n\".join([f\"{i}. [{proj.project_type}] {proj.long_desc[:400] if proj.long_desc else \\\\\"No description\\\\\"}\" for i, proj in enumerate(profile.production_projects[:10], 1)])}\n",
        "\n",
        "CRITICAL: You MUST include both \"SUMMARY:\" and \"SKILLS:\" headers.\"\"\"\n",
        "                prompt_texts.append(prompt)\n",
        "            \n",
        "            # Call LLM in batch\n",
        "            try:\n",
        "                llm_responses = await llm_client.generate_batch(prompt_texts, max_concurrent=batch_size)\n",
        "                \n",
        "                # Parse responses\n",
        "                for profile, llm_response in zip(profiles_with_descriptions, llm_responses):\n",
        "                    try:\n",
        "                        if not llm_response.startswith(\"Error:\"):\n",
        "                            summary_text, skills_list = parse_llm_response(llm_response)\n",
        "                            profile.extracted_skills = skills_list\n",
        "                            if skills_list:\n",
        "                                profile.intelligence_summary = f\"{summary_text}\\\\n\\\\nSkills: {\", \".join(skills_list)}\"\n",
        "                            else:\n",
        "                                profile.intelligence_summary = summary_text\n",
        "                        else:\n",
        "                            profile.intelligence_summary = llm_response\n",
        "                            profile.extracted_skills = []\n",
        "                        \n",
        "                        repo.upsert_contributor(profile)\n",
        "                        processed += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Failed to process profile: {e}\")\n",
        "                        failed += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Batch LLM generation failed: {e}\")\n",
        "                failed += len(profiles_with_descriptions)\n",
        "        \n",
        "        # Update profiles WITHOUT descriptions\n",
        "        for profile, summary in profiles_without_descriptions:\n",
        "            try:\n",
        "                profile.intelligence_summary = summary\n",
        "                repo.upsert_contributor(profile)\n",
        "                processed += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to update profile: {e}\")\n",
        "                failed += 1\n",
        "    \n",
        "    print(f\"Done! Processed: {processed}, Failed: {failed}\")\n",
        "\n",
        "asyncio.run(profile_extraction())\n",
        "'''\n",
        "\n",
        "with open('/tmp/profile_extraction.py', 'w') as f:\n",
        "    f.write(profiling_script)\n",
        "\n",
        "print(\"✅ Profiling script created\")\n",
        "print(\"🔥 Running py-spy profiler...\")\n",
        "print(\"   Capturing ALL function calls...\")\n",
        "print(\"\")\n",
        "\n",
        "# Run py-spy\n",
        "result = subprocess.run([\n",
        "    'py-spy', 'record',\n",
        "    '--format', 'speedscope',\n",
        "    '--output', '/tmp/flamegraph.speedscope.json',\n",
        "    '--rate', '100',\n",
        "    '--',\n",
        "    'python3', '/tmp/profile_extraction.py'\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n✅ Profiling complete!\")\n",
        "    print(\"\\n📊 Flamegraph: /tmp/flamegraph.speedscope.json\")\n",
        "    print(\"\\n🌐 To visualize:\")\n",
        "    print(\"   1. Download the file (see next cell)\")\n",
        "    print(\"   2. Go to: https://www.speedscope.app/\")\n",
        "    print(\"   3. Drag & drop the JSON\")\n",
        "    \n",
        "    file_size = os.path.getsize('/tmp/flamegraph.speedscope.json')\n",
        "    print(f\"\\n📦 Size: {file_size / 1024:.1f} KB\")\n",
        "else:\n",
        "    print(f\"\\n❌ Error:\\n{result.stderr}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Download Flamegraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import FileLink, display\n",
        "\n",
        "flamegraph = '/tmp/flamegraph.speedscope.json'\n",
        "\n",
        "if os.path.exists(flamegraph):\n",
        "    print(\"📥 Download your flamegraph:\")\n",
        "    display(FileLink(flamegraph))\n",
        "    print(\"\\n🌐 Then visit: https://www.speedscope.app/\")\n",
        "    print(\"   Drag & drop to visualize!\")\n",
        "else:\n",
        "    print(\"❌ Run cell 8 first to generate flamegraph\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Stop All Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🛑 Stopping services...\")\n",
        "\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"✅ Ngrok stopped\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'streamlit'], capture_output=True)\n",
        "print(\"✅ Streamlit stopped\")\n",
        "\n",
        "subprocess.run(['pkill', 'ollama'], capture_output=True)\n",
        "print(\"✅ Ollama stopped\")\n",
        "\n",
        "print(\"\\n✅ All services stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
