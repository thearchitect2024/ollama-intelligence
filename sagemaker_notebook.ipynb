{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contributor Intelligence Platform - SageMaker Studio\n",
        "## File-Based Database (SQLite) Version\n",
        "\n",
        "This notebook processes contributor data and extracts intelligence using LLMs in SageMaker Studio with a local SQLite database.\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 Required Files to Upload to SageMaker Studio\n",
        "\n",
        "Before running this notebook, make sure you've uploaded:\n",
        "\n",
        "### Required:\n",
        "1. ✅ **This notebook** (`sagemaker_notebook.ipynb`)\n",
        "2. ✅ **Your CSV file** (e.g., `contributor_data.csv`)\n",
        "   - Update the path in Cell 4 below\n",
        "\n",
        "### Optional (for easier setup):\n",
        "3. `setup_sagemaker.sh` - Automated setup script (run in terminal)\n",
        "4. `test_sagemaker_setup.py` - Verify setup before processing\n",
        "\n",
        "### Alternative:\n",
        "- Instead of this notebook, you can use `sagemaker_script.py` (standalone Python script)\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Quick Setup Steps\n",
        "\n",
        "**Option A: Use Setup Script (Recommended)**\n",
        "```bash\n",
        "# In SageMaker terminal:\n",
        "bash setup_sagemaker.sh\n",
        "```\n",
        "\n",
        "**Option B: Manual Setup (follow cells below)**\n",
        "- Run cells 2-3 to install Ollama and dependencies\n",
        "- Then continue with the rest of the notebook\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Performance Note\n",
        "\n",
        "- **CPU (ml.t3.xlarge)**: ~0.4 profiles/min\n",
        "- **GPU (ml.g4dn.xlarge)**: ~2-3 profiles/min (5-8x faster!)\n",
        "- **GPU (ml.g5.xlarge)**: ~5-7 profiles/min (12-15x faster!)\n",
        "\n",
        "For 3,200 profiles: CPU = 2 hours | GPU = 30 minutes\n",
        "\n",
        "💡 **Recommendation**: Use `ml.g4dn.xlarge` for best value!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -q pandas pydantic pydantic-settings python-dotenv tqdm aiohttp\n",
        "echo \"✅ Dependencies installed\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# File paths\n",
        "SQLITE_DB_PATH = '/tmp/contributor_intelligence.db'\n",
        "CSV_FILE_PATH = '/home/sagemaker-user/contributor_data.csv'  # Update with your CSV path\n",
        "\n",
        "# LLM Configuration\n",
        "OLLAMA_MODEL = 'qwen2.5:7b-instruct-q4_0'\n",
        "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
        "\n",
        "# Processing configuration\n",
        "MAX_CONCURRENT = 10  # Number of parallel LLM requests\n",
        "CHUNK_SIZE = 500  # Rows to process at once\n",
        "\n",
        "print(f\"✅ Configuration set\")\n",
        "print(f\"   Database: {SQLITE_DB_PATH}\")\n",
        "print(f\"   CSV: {CSV_FILE_PATH}\")\n",
        "print(f\"   LLM: Ollama Local ({OLLAMA_MODEL})\")\n",
        "print(f\"   Concurrent Requests: {MAX_CONCURRENT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Check if Ollama is already installed\n",
        "if command -v ollama &> /dev/null; then\n",
        "    echo \"✅ Ollama already installed: $(ollama --version)\"\n",
        "else\n",
        "    echo \"📦 Installing Ollama...\"\n",
        "    curl -fsSL https://ollama.com/install.sh | sh\n",
        "    echo \"✅ Ollama installed\"\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Stop any existing Ollama processes\n",
        "pkill ollama || true\n",
        "sleep 2\n",
        "\n",
        "# Start Ollama with optimized settings for parallel processing\n",
        "echo \"🚀 Starting Ollama server...\"\n",
        "OLLAMA_NUM_PARALLEL=10 OLLAMA_MAX_LOADED_MODELS=1 ollama serve > /tmp/ollama.log 2>&1 &\n",
        "echo \"Ollama PID: $!\"\n",
        "sleep 5\n",
        "\n",
        "# Download the model (this may take 5-10 minutes for first run)\n",
        "echo \"📥 Downloading model (this may take a few minutes)...\"\n",
        "ollama pull qwen2.5:7b-instruct-q4_0\n",
        "\n",
        "echo \"✅ Ollama ready!\"\n",
        "echo \"📊 Check logs: tail -f /tmp/ollama.log\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create SQLite Database Manager\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import json\n",
        "from contextlib import contextmanager\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SQLiteDatabaseManager:\n",
        "    \"\"\"SQLite database manager for SageMaker.\"\"\"\n",
        "    \n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self._initialize_database()\n",
        "    \n",
        "    def _initialize_database(self):\n",
        "        \"\"\"Create tables if they don't exist.\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            conn.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS contributors (\n",
        "                    email TEXT PRIMARY KEY,\n",
        "                    contributor_id TEXT UNIQUE NOT NULL,\n",
        "                    processed_data TEXT NOT NULL,\n",
        "                    intelligence_summary TEXT,\n",
        "                    processing_status TEXT DEFAULT 'pending',\n",
        "                    error_message TEXT,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    intelligence_extracted_at TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "            conn.execute('CREATE INDEX IF NOT EXISTS idx_status ON contributors(processing_status)')\n",
        "            conn.execute('CREATE INDEX IF NOT EXISTS idx_created ON contributors(created_at)')\n",
        "            conn.commit()\n",
        "        logger.info(f\"SQLite database initialized: {self.db_path}\")\n",
        "    \n",
        "    def execute_query(self, query: str, params: tuple = None, fetch_one: bool = False, fetch_all: bool = False):\n",
        "        \"\"\"Execute a query and return results.\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        conn.row_factory = sqlite3.Row\n",
        "        cursor = conn.cursor()\n",
        "        try:\n",
        "            cursor.execute(query, params or ())\n",
        "            if fetch_one:\n",
        "                row = cursor.fetchone()\n",
        "                return dict(row) if row else None\n",
        "            elif fetch_all:\n",
        "                return [dict(row) for row in cursor.fetchall()]\n",
        "            conn.commit()\n",
        "            return None\n",
        "        finally:\n",
        "            cursor.close()\n",
        "            conn.close()\n",
        "\n",
        "# Initialize database\n",
        "db_manager = SQLiteDatabaseManager(SQLITE_DB_PATH)\n",
        "print(f\"✅ Database initialized: {SQLITE_DB_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Define Data Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "class ProjectInfo(BaseModel):\n",
        "    name: str\n",
        "    description: Optional[str] = None\n",
        "    hours: float = 0.0\n",
        "    environment: Optional[str] = None\n",
        "\n",
        "class ActivitySummary(BaseModel):\n",
        "    total_hours: float = 0.0\n",
        "    total_projects: int = 0\n",
        "    production_projects: int = 0\n",
        "    avg_hours_per_project: float = 0.0\n",
        "\n",
        "class ContributorProfile(BaseModel):\n",
        "    email: str\n",
        "    contributor_id: str\n",
        "    country: Optional[str] = None\n",
        "    us_state: Optional[str] = None\n",
        "    risk_tier: Optional[str] = None\n",
        "    kyc_status: Optional[str] = None\n",
        "    dots_status: Optional[str] = None\n",
        "    languages: List[str] = Field(default_factory=list)\n",
        "    qualifications: Optional[str] = None\n",
        "    projects: List[ProjectInfo] = Field(default_factory=list)\n",
        "    activity_summary: ActivitySummary = Field(default_factory=ActivitySummary)\n",
        "    skills: List[str] = Field(default_factory=list)\n",
        "    intelligence_summary: Optional[str] = None\n",
        "\n",
        "print(\"✅ Data models defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def parse_projects_json(projects_str: str) -> List[ProjectInfo]:\n",
        "    \"\"\"Parse projects JSON string.\"\"\"\n",
        "    if not projects_str or pd.isna(projects_str):\n",
        "        return []\n",
        "    try:\n",
        "        projects_data = json.loads(projects_str)\n",
        "        projects = []\n",
        "        for p in projects_data:\n",
        "            if p.get('environment') in ['Production', 'production']:\n",
        "                projects.append(ProjectInfo(\n",
        "                    name=p.get('project_name', 'Unknown'),\n",
        "                    description=p.get('description'),\n",
        "                    hours=float(p.get('hours', 0)),\n",
        "                    environment=p.get('environment')\n",
        "                ))\n",
        "        return projects\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def parse_languages(languages_str: str) -> List[str]:\n",
        "    \"\"\"Parse languages JSON string.\"\"\"\n",
        "    if not languages_str or pd.isna(languages_str):\n",
        "        return []\n",
        "    try:\n",
        "        return json.loads(languages_str) if isinstance(languages_str, str) else []\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def process_contributor_row(row: pd.Series) -> ContributorProfile:\n",
        "    \"\"\"Process a single CSV row into a ContributorProfile.\"\"\"\n",
        "    email = str(row.get('email', '')).strip().lower()\n",
        "    if not email:\n",
        "        raise ValueError(\"Missing email\")\n",
        "    \n",
        "    projects = parse_projects_json(str(row.get('projects_info', row.get('projects_json', ''))))\n",
        "    languages = parse_languages(str(row.get('languages_known', row.get('languages_json', ''))))\n",
        "    \n",
        "    activity = ActivitySummary(\n",
        "        total_hours=sum(p.hours for p in projects),\n",
        "        total_projects=len(projects),\n",
        "        production_projects=len(projects),\n",
        "        avg_hours_per_project=sum(p.hours for p in projects) / len(projects) if projects else 0\n",
        "    )\n",
        "    \n",
        "    return ContributorProfile(\n",
        "        email=email,\n",
        "        contributor_id=str(row.get('contributor_id', email.split('@')[0])),\n",
        "        country=str(row.get('currently_residing_country__c', '')) if pd.notna(row.get('currently_residing_country__c')) else None,\n",
        "        us_state=str(row.get('currently_residing_us_state__c', '')) if pd.notna(row.get('currently_residing_us_state__c')) else None,\n",
        "        risk_tier=str(row.get('risk_tier__c', '')) if pd.notna(row.get('risk_tier__c')) else None,\n",
        "        kyc_status=str(row.get('kyc_status__c', '')) if pd.notna(row.get('kyc_status__c')) else None,\n",
        "        dots_status=str(row.get('dots_status__c', '')) if pd.notna(row.get('dots_status__c')) else None,\n",
        "        languages=languages,\n",
        "        qualifications=str(row.get('qualifications_summary', '')) if pd.notna(row.get('qualifications_summary')) else None,\n",
        "        projects=projects,\n",
        "        activity_summary=activity\n",
        "    )\n",
        "\n",
        "print(\"✅ Processing functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Database Repository Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upsert_contributor(profile: ContributorProfile):\n",
        "    \"\"\"Insert or update contributor in database.\"\"\"\n",
        "    processed_data = json.dumps(profile.dict())\n",
        "    query = '''\n",
        "        INSERT OR REPLACE INTO contributors \n",
        "        (email, contributor_id, processed_data, updated_at)\n",
        "        VALUES (?, ?, ?, CURRENT_TIMESTAMP)\n",
        "    '''\n",
        "    db_manager.execute_query(query, (profile.email, profile.contributor_id, processed_data))\n",
        "\n",
        "def get_pending_contributors() -> List[dict]:\n",
        "    \"\"\"Get contributors without intelligence summaries.\"\"\"\n",
        "    query = '''\n",
        "        SELECT email, contributor_id, processed_data \n",
        "        FROM contributors \n",
        "        WHERE intelligence_summary IS NULL OR intelligence_summary = ''\n",
        "    '''\n",
        "    results = db_manager.execute_query(query, fetch_all=True)\n",
        "    for result in results:\n",
        "        result['processed_data'] = json.loads(result['processed_data'])\n",
        "    return results\n",
        "\n",
        "def update_intelligence(email: str, summary: str, skills: List[str]):\n",
        "    \"\"\"Update contributor with intelligence summary.\"\"\"\n",
        "    existing = db_manager.execute_query(\n",
        "        'SELECT processed_data FROM contributors WHERE email = ?',\n",
        "        (email,),\n",
        "        fetch_one=True\n",
        "    )\n",
        "    if existing:\n",
        "        data = json.loads(existing['processed_data'])\n",
        "        data['intelligence_summary'] = summary\n",
        "        data['skills'] = skills\n",
        "        query = '''\n",
        "            UPDATE contributors \n",
        "            SET intelligence_summary = ?,\n",
        "                processed_data = ?,\n",
        "                intelligence_extracted_at = CURRENT_TIMESTAMP,\n",
        "                processing_status = 'completed'\n",
        "            WHERE email = ?\n",
        "        '''\n",
        "        db_manager.execute_query(query, (summary, json.dumps(data), email))\n",
        "\n",
        "print(\"✅ Repository functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. LLM Client (Ollama)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "\n",
        "class SimpleLLMClient:\n",
        "    \"\"\"Simple async LLM client for Ollama.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_url: str, model: str):\n",
        "        self.base_url = base_url\n",
        "        self.model = model\n",
        "    \n",
        "    async def generate_async(self, prompt: str) -> str:\n",
        "        \"\"\"Generate text from prompt asynchronously.\"\"\"\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            try:\n",
        "                async with session.post(\n",
        "                    f\"{self.base_url}/api/generate\",\n",
        "                    json={\n",
        "                        \"model\": self.model,\n",
        "                        \"prompt\": prompt,\n",
        "                        \"stream\": False,\n",
        "                        \"options\": {\n",
        "                            \"temperature\": 0.05,\n",
        "                            \"top_p\": 0.9,\n",
        "                            \"num_predict\": 320\n",
        "                        }\n",
        "                    },\n",
        "                    timeout=aiohttp.ClientTimeout(total=60)\n",
        "                ) as response:\n",
        "                    result = await response.json()\n",
        "                    return result.get('response', '')\n",
        "            except Exception as e:\n",
        "                logger.error(f\"LLM generation error: {e}\")\n",
        "                return \"\"\n",
        "    \n",
        "    async def generate_batch(self, prompts: List[str], max_concurrent: int = 10) -> List[str]:\n",
        "        \"\"\"Generate text for multiple prompts concurrently.\"\"\"\n",
        "        semaphore = asyncio.Semaphore(max_concurrent)\n",
        "        \n",
        "        async def process_with_limit(prompt: str) -> str:\n",
        "            async with semaphore:\n",
        "                return await self.generate_async(prompt)\n",
        "        \n",
        "        tasks = [process_with_limit(p) for p in prompts]\n",
        "        return await asyncio.gather(*tasks, return_exceptions=False)\n",
        "\n",
        "# Initialize LLM client\n",
        "llm_client = SimpleLLMClient(OLLAMA_BASE_URL, OLLAMA_MODEL)\n",
        "print(\"✅ LLM client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def create_prompt(contributor: dict) -> Optional[str]:\n",
        "    \"\"\"Create prompt for LLM.\"\"\"\n",
        "    data = contributor['processed_data']\n",
        "    projects = data.get('projects', [])\n",
        "    \n",
        "    if not projects:\n",
        "        return None\n",
        "    \n",
        "    descriptions = [p['description'] for p in projects if p.get('description')]\n",
        "    \n",
        "    if not descriptions:\n",
        "        return None\n",
        "    \n",
        "    projects_text = \"\\n\".join([f\"- {desc}\" for desc in descriptions[:5]])\n",
        "    \n",
        "    prompt = f\"\"\"Based on the following project descriptions, write a professional 150-word summary of this contributor's work and extract 3-5 key skills.\n",
        "\n",
        "Project Descriptions:\n",
        "{projects_text}\n",
        "\n",
        "Format your response exactly as:\n",
        "SUMMARY: [150-word professional summary]\n",
        "SKILLS: [skill1, skill2, skill3]\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def parse_llm_response(response: str) -> tuple:\n",
        "    \"\"\"Parse LLM response into summary and skills.\"\"\"\n",
        "    summary = \"\"\n",
        "    skills = []\n",
        "    \n",
        "    # Extract summary\n",
        "    summary_match = re.search(r'SUMMARY:\\s*(.+?)(?=SKILLS:|$)', response, re.DOTALL | re.IGNORECASE)\n",
        "    if summary_match:\n",
        "        summary = summary_match.group(1).strip()\n",
        "    \n",
        "    # Extract skills\n",
        "    skills_match = re.search(r'SKILLS:\\s*(.+)', response, re.DOTALL | re.IGNORECASE)\n",
        "    if skills_match:\n",
        "        skills_text = skills_match.group(1).strip()\n",
        "        skills = [s.strip() for s in re.split(r'[,\\n]', skills_text) if s.strip()]\n",
        "        skills = [s.strip('[]\"\\' ') for s in skills[:5]]\n",
        "    \n",
        "    return summary, skills\n",
        "\n",
        "print(\"✅ Intelligence extraction functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "t "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(f\"📥 Loading CSV: {CSV_FILE_PATH}\")\n",
        "\n",
        "if not os.path.exists(CSV_FILE_PATH):\n",
        "    print(f\"❌ CSV file not found: {CSV_FILE_PATH}\")\n",
        "    print(\"   Please update CSV_FILE_PATH in cell 4\")\n",
        "else:\n",
        "    total_processed = 0\n",
        "    total_failed = 0\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Read and process CSV in chunks\n",
        "    for chunk_num, chunk_df in enumerate(pd.read_csv(CSV_FILE_PATH, chunksize=CHUNK_SIZE)):\n",
        "        print(f\"\\n📦 Processing chunk {chunk_num + 1} ({len(chunk_df)} rows)...\")\n",
        "        \n",
        "        for idx, row in tqdm(chunk_df.iterrows(), total=len(chunk_df), desc=\"Processing\"):\n",
        "            try:\n",
        "                profile = process_contributor_row(row)\n",
        "                upsert_contributor(profile)\n",
        "                total_processed += 1\n",
        "            except Exception as e:\n",
        "                total_failed += 1\n",
        "                if total_failed <= 5:  # Only show first 5 errors\n",
        "                    print(f\"\\n⚠️  Row {idx} error: {str(e)[:100]}\")\n",
        "    \n",
        "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
        "    \n",
        "    print(f\"\\n✅ CSV Import Complete!\")\n",
        "    print(f\"   Processed: {total_processed}\")\n",
        "    print(f\"   Failed: {total_failed}\")\n",
        "    print(f\"   Time: {elapsed:.1f} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_intelligence_extraction():\n",
        "    \"\"\"Process intelligence extraction for all pending contributors.\"\"\"\n",
        "    pending = get_pending_contributors()\n",
        "    \n",
        "    if not pending:\n",
        "        print(\"✅ No pending contributors - all done!\")\n",
        "        return 0, 0\n",
        "    \n",
        "    processed = 0\n",
        "    failed = 0\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    print(f\"🚀 Processing {len(pending)} contributors with {MAX_CONCURRENT} concurrent requests...\")\n",
        "    \n",
        "    # Process in batches\n",
        "    for i in range(0, len(pending), MAX_CONCURRENT):\n",
        "        batch = pending[i:i+MAX_CONCURRENT]\n",
        "        \n",
        "        prompts = []\n",
        "        valid_contributors = []\n",
        "        \n",
        "        for contributor in batch:\n",
        "            prompt = create_prompt(contributor)\n",
        "            if prompt:\n",
        "                prompts.append(prompt)\n",
        "                valid_contributors.append(contributor)\n",
        "            else:\n",
        "                # No projects/descriptions - use generic summary\n",
        "                update_intelligence(\n",
        "                    contributor['email'],\n",
        "                    \"Contributor with limited project information available.\",\n",
        "                    []\n",
        "                )\n",
        "                processed += 1\n",
        "        \n",
        "        if prompts:\n",
        "            try:\n",
        "                # Generate summaries\n",
        "                responses = await llm_client.generate_batch(prompts, MAX_CONCURRENT)\n",
        "                \n",
        "                # Parse and save results\n",
        "                for contributor, response in zip(valid_contributors, responses):\n",
        "                    try:\n",
        "                        summary, skills = parse_llm_response(response)\n",
        "                        if summary:\n",
        "                            update_intelligence(contributor['email'], summary, skills)\n",
        "                            processed += 1\n",
        "                        else:\n",
        "                            failed += 1\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error processing {contributor['email']}: {e}\")\n",
        "                        failed += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Batch processing error: {e}\")\n",
        "                failed += len(prompts)\n",
        "        \n",
        "        # Progress update\n",
        "        total_done = min(i + MAX_CONCURRENT, len(pending))\n",
        "        elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
        "        speed = total_done / elapsed if elapsed > 0 else 0\n",
        "        remaining = (len(pending) - total_done) / speed if speed > 0 else 0\n",
        "        \n",
        "        print(f\"\\r⚡ Progress: {total_done}/{len(pending)} | \"\n",
        "              f\"Speed: {speed:.1f} profiles/min | \"\n",
        "              f\"ETA: {remaining:.1f} min\", end='')\n",
        "    \n",
        "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
        "    print(f\"\\n\\n✅ Extraction Complete!\")\n",
        "    print(f\"   Successful: {processed}\")\n",
        "    print(f\"   Failed: {failed}\")\n",
        "    print(f\"   Time: {elapsed:.1f} minutes\")\n",
        "    print(f\"   Speed: {processed/elapsed:.2f} profiles/min\" if elapsed > 0 else \"\")\n",
        "    \n",
        "    return processed, failed\n",
        "\n",
        "# Run the extraction\n",
        "processed_count, failed_count = await process_intelligence_extraction()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. View Results and Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get database statistics\n",
        "stats = db_manager.execute_query(\n",
        "    '''\n",
        "    SELECT \n",
        "        COUNT(*) as total,\n",
        "        SUM(CASE WHEN intelligence_summary IS NOT NULL THEN 1 ELSE 0 END) as with_intelligence,\n",
        "        SUM(CASE WHEN intelligence_summary IS NULL THEN 1 ELSE 0 END) as pending\n",
        "    FROM contributors\n",
        "    ''',\n",
        "    fetch_one=True\n",
        ")\n",
        "\n",
        "print(\"📊 Database Statistics:\")\n",
        "print(f\"   Total Contributors: {stats['total']}\")\n",
        "print(f\"   With Intelligence: {stats['with_intelligence']}\")\n",
        "print(f\"   Pending: {stats['pending']}\")\n",
        "if stats['total'] > 0:\n",
        "    print(f\"   Completion: {stats['with_intelligence']/stats['total']*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. View Sample Profiles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View sample profiles\n",
        "samples = db_manager.execute_query(\n",
        "    'SELECT email, intelligence_summary, processed_data FROM contributors WHERE intelligence_summary IS NOT NULL LIMIT 5',\n",
        "    fetch_all=True\n",
        ")\n",
        "\n",
        "print(\"\\n📋 Sample Profiles:\\n\")\n",
        "\n",
        "for sample in samples:\n",
        "    data = json.loads(sample['processed_data'])\n",
        "    print(\"=\"*80)\n",
        "    print(f\"📧 Email: {sample['email']}\")\n",
        "    print(f\"🎯 Skills: {', '.join(data.get('skills', [])[:5])}\")\n",
        "    print(f\"📝 Summary: {sample['intelligence_summary'][:200]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Export Results to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to CSV\n",
        "all_contributors = db_manager.execute_query(\n",
        "    'SELECT email, contributor_id, intelligence_summary, processed_data FROM contributors',\n",
        "    fetch_all=True\n",
        ")\n",
        "\n",
        "export_data = []\n",
        "for c in all_contributors:\n",
        "    data = json.loads(c['processed_data'])\n",
        "    export_data.append({\n",
        "        'email': c['email'],\n",
        "        'contributor_id': c['contributor_id'],\n",
        "        'intelligence_summary': c['intelligence_summary'],\n",
        "        'skills': ', '.join(data.get('skills', [])),\n",
        "        'total_projects': data['activity_summary']['total_projects'],\n",
        "        'total_hours': data['activity_summary']['total_hours']\n",
        "    })\n",
        "\n",
        "export_df = pd.DataFrame(export_data)\n",
        "export_path = '/tmp/contributor_intelligence_results.csv'\n",
        "export_df.to_csv(export_path, index=False)\n",
        "\n",
        "print(f\"✅ Results exported to: {export_path}\")\n",
        "print(f\"   Total records: {len(export_df)}\")\n",
        "print(f\"\\n📥 Download this file from SageMaker Studio file browser!\")\n",
        "print(f\"   Or copy to S3: aws s3 cp {export_path} s3://your-bucket/\")\n",
        "\n",
        "# Show preview\n",
        "print(f\"\\n📊 Preview of results:\")\n",
        "export_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Check System Information (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "echo \"🖥️  System Information:\"\n",
        "echo \"\"\n",
        "\n",
        "# Check for GPU\n",
        "if command -v nvidia-smi &> /dev/null; then\n",
        "    echo \"GPU Information:\"\n",
        "    nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv\n",
        "else\n",
        "    echo \"ℹ️  No GPU detected - using CPU\"\n",
        "    echo \"   For faster processing, use ml.g4dn.xlarge or ml.g5.xlarge\"\n",
        "fi\n",
        "\n",
        "echo \"\"\n",
        "echo \"CPU Information:\"\n",
        "nproc\n",
        "cat /proc/cpuinfo | grep \"model name\" | head -1\n",
        "\n",
        "echo \"\"\n",
        "echo \"Memory Information:\"\n",
        "free -h | head -2\n",
        "\n",
        "echo \"\"\n",
        "echo \"Disk Space:\"\n",
        "df -h /tmp | tail -1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ✅ Processing Complete!\n",
        "\n",
        "### 📥 Your Results\n",
        "\n",
        "Your processed data is now available:\n",
        "\n",
        "1. **CSV Export**: `/tmp/contributor_intelligence_results.csv`\n",
        "   - Download from file browser\n",
        "   - Or copy to S3: `aws s3 cp /tmp/contributor_intelligence_results.csv s3://your-bucket/`\n",
        "\n",
        "2. **SQLite Database**: `/tmp/contributor_intelligence.db`\n",
        "   - Contains all raw data\n",
        "   - Can query directly with SQL\n",
        "\n",
        "### 🎉 What You Got\n",
        "\n",
        "- ✅ Structured contributor data in database\n",
        "- ✅ AI-generated intelligence summaries\n",
        "- ✅ Extracted skills for each contributor\n",
        "- ✅ Complete CSV export ready to use\n",
        "\n",
        "### 🚀 Next Steps\n",
        "\n",
        "1. **Download results** - Get the CSV file\n",
        "2. **Analyze data** - Open in Excel or your favorite tool\n",
        "3. **Build visualizations** - Create dashboards\n",
        "4. **Scale up** - For production, use RDS PostgreSQL\n",
        "5. **Deploy** - Create API endpoints or Streamlit UI\n",
        "\n",
        "### 💡 Tips\n",
        "\n",
        "- **Stop Ollama** when done: `pkill ollama`\n",
        "- **Clean up** temp files: `rm /tmp/*.db /tmp/*.csv`\n",
        "- **Save notebook** - Don't lose your work!\n",
        "- **Document changes** - Note any modifications you made\n",
        "\n",
        "Thank you for using the Contributor Intelligence Platform! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
